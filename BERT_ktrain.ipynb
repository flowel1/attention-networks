{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_ktrain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuGIPjap5Sdx",
        "colab_type": "text"
      },
      "source": [
        "## **BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B8D5KLM5c-F",
        "colab_type": "text"
      },
      "source": [
        "BERT (Bidirectional Encoder Representations from Transformer) and its descendants are currently state-of-the-art models for nearly all NLP tasks.\n",
        "\n",
        "Released by Google in 2019, BERT builds powerful context-aware representations of words that can be exploited to perform custom classification tasks.\n",
        "\n",
        "For further details: https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fExW2GA6Sfr",
        "colab_type": "text"
      },
      "source": [
        "**Fine-tuning BERT for text classification with ktrain**\n",
        "\n",
        "This notebook fine-tunes BERT for a custom text classification task. Simply, a classification layer is added on top of BERT and the whole model is retrained starting from pre-trained BERT weights.\n",
        "\n",
        "With ktrain, we can do all this with just a few lines of code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jrq106EU5Ng6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip3 install ktrain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCXywW4Euc2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "# if you want to run the notebook from scratch\n",
        "import shutil\n",
        "shutil.rmtree('/content/data')\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCwpIiSFzUKT",
        "colab_type": "text"
      },
      "source": [
        "**STEP 1**: load and preprocess the data\n",
        "\n",
        "This is actually the hardest part of the job when you are using ktrain :)\n",
        "\n",
        "Train / test data must be stored in folders with a specific structure in order to be processed by the ktrain methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8oNcIhA5cH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "df = pd.read_csv(\"articles_topics.csv\")[['title', 'topic']]\n",
        "\n",
        "dfTrain = []\n",
        "dfTest  = []\n",
        "for topic in df.topic.unique():\n",
        "    data = df.query(\"topic == '{}'\".format(topic))\n",
        "    dfTrain += data[:16].to_dict(orient = 'rows')   # keep just 16 train + 4 test examples for each topic (to balance the dataset)\n",
        "    dfTest  += data[16:20].to_dict(orient = 'rows')\n",
        "dfTrain = pd.DataFrame(dfTrain)\n",
        "dfTest  = pd.DataFrame(dfTest)\n",
        "\n",
        "main_dir = os.getcwd()\n",
        "\n",
        "labels = sorted(dfTrain.topic.unique())\n",
        "\n",
        "def data_to_folders(df, labels, to_folder):\n",
        "    for label in labels:\n",
        "        current_dir = os.path.join(to_folder, label)\n",
        "        os.mkdir(current_dir)\n",
        "        for ix, row in df.query(\"topic == '{}'\".format(label)).iterrows():\n",
        "            with open(os.path.join(current_dir, '{}.txt'.format(ix)), 'w') as f:\n",
        "                f.write(row['title'])\n",
        "\n",
        "for dataset, dataset_id in [(dfTrain, 'train'), (dfTest, 'test')]:\n",
        "    to_folder = os.path.join(main_dir, 'data', dataset_id)\n",
        "    os.makedirs(to_folder)\n",
        "    data_to_folders(dataset, labels, to_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auXT_JGwzgyb",
        "colab_type": "text"
      },
      "source": [
        "**STEP 2**: do the fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOQYcefz62kU",
        "colab_type": "code",
        "outputId": "c34ed872-3dd9-47b3-b2ec-469643fab4ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "import ktrain\n",
        "from ktrain import text\n",
        "\n",
        "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_folder(os.path.join(main_dir, 'data'),\n",
        "                                                                       maxlen  = 30,\n",
        "                                                                       preprocess_mode = 'bert',\n",
        "                                                                       classes = labels)\n",
        "\n",
        "model = text.text_classifier('bert', (x_train, y_train), preproc = preproc)\n",
        "learner = ktrain.get_learner(model, train_data = (x_train, y_train), val_data = (x_test, y_test), batch_size = 6)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using Keras version: 2.2.4-tf\n",
            "detected encoding: utf-8\n",
            "downloading pretrained BERT model (multi_cased_L-12_H-768_A-12.zip)...\n",
            "[██████████████████████████████████████████████████]\n",
            "extracting pretrained BERT model...\n",
            "done.\n",
            "\n",
            "cleanup downloaded zip...\n",
            "done.\n",
            "\n",
            "preprocessing train...\n",
            "language: it\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "preprocessing test...\n",
            "language: it\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 30\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "118ompXTFSfg",
        "colab_type": "code",
        "outputId": "2b81283c-7d1d-4c96-92e9-4cbf68d23392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "learner.fit_onecycle(lr = 2e-5, epochs = 10, checkpoint_folder = os.path.join(main_dir, 'checkpoints'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 2e-05...\n",
            "Train on 160 samples, validate on 40 samples\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 51s 321ms/sample - loss: 2.3687 - accuracy: 0.1063 - val_loss: 2.2448 - val_accuracy: 0.2000\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 29s 184ms/sample - loss: 2.1532 - accuracy: 0.2125 - val_loss: 2.0944 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 29s 182ms/sample - loss: 1.7723 - accuracy: 0.5312 - val_loss: 1.6147 - val_accuracy: 0.5500\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 30s 185ms/sample - loss: 1.0620 - accuracy: 0.7875 - val_loss: 1.2090 - val_accuracy: 0.7000\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 30s 185ms/sample - loss: 0.5967 - accuracy: 0.8687 - val_loss: 1.4061 - val_accuracy: 0.5750\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 29s 183ms/sample - loss: 0.3765 - accuracy: 0.9563 - val_loss: 0.9110 - val_accuracy: 0.7250\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 29s 183ms/sample - loss: 0.2009 - accuracy: 0.9563 - val_loss: 0.8450 - val_accuracy: 0.7750\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 30s 185ms/sample - loss: 0.0899 - accuracy: 1.0000 - val_loss: 0.7515 - val_accuracy: 0.8000\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 29s 182ms/sample - loss: 0.0590 - accuracy: 1.0000 - val_loss: 0.7264 - val_accuracy: 0.7750\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 32s 202ms/sample - loss: 0.0503 - accuracy: 1.0000 - val_loss: 0.7103 - val_accuracy: 0.8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5ef407fcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Sci33b4VzbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.model.load_weights(os.path.join(main_dir, 'checkpoints', 'weights-08.hdf5'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow88xtON0HMg",
        "colab_type": "text"
      },
      "source": [
        "The model is now fine-tuned on our classification task. Below we visualize our predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb_8wyPUrmjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# retrieve labels\n",
        "import numpy as np\n",
        "dfTest.loc[:, 'encoding'] = np.argmax(y_test, axis = 1)\n",
        "labels_dict = dfTest[['topic', 'encoding']].drop_duplicates()\n",
        "labels_dict = {row['encoding'] : row['topic'] for _, row in dfTest.iterrows()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5fyXpZJqoWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = learner.predict((x_test, y_test))\n",
        "predictions = list(map(lambda i : labels_dict[i], np.argmax(predictions, axis = 1)))\n",
        "\n",
        "dfTest.loc[:, 'prediction'] = predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tTYYxkAqxeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrap(text, maxlen =  max(map(len, labels_dict.values()))):\n",
        "    return text+(' ' * (maxlen - len(text)))\n",
        "\n",
        "print(\"{}\\t{}\\t{}\".format(wrap('GROUND TRUTH'), wrap('PREDICTION'), 'TITLE'))\n",
        "for _, row in dfTest.iterrows():\n",
        "    print(\"{}\\t{}\\t{}\".format(wrap(row['topic']), wrap(row['prediction']), row['title']))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}